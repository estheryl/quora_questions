{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C-NFtqd-ZLP0"
   },
   "source": [
    "# Quora Question Pair with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4y869aTZLP4"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rpcRWWXZLP8"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "WtQ0XsGtZXIB",
    "outputId": "9dfe0c15-d4ad-40bd-c44d-0a100eca37ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /data\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g5-64P63bjcc",
    "outputId": "427580c2-ac56-4a89-d95f-45be244e7c38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/My Drive/data\n"
     ]
    }
   ],
   "source": [
    "cd ../data/'My Drive'/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "dGdkG6kkZLQB",
    "outputId": "3707169e-099b-4565-bd15-074fccab9353"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2718: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LuQb0aAiZLQN"
   },
   "outputs": [],
   "source": [
    "train_df = train_df.fillna('None')\n",
    "test_df = test_df.fillna('None')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zaHARrChZLQQ"
   },
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlE0w4sBZLQR"
   },
   "outputs": [],
   "source": [
    "# from https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        #text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    text = text.split()\n",
    "    return(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mr44QrRVZLQU"
   },
   "outputs": [],
   "source": [
    "def get_vocab(list_of_content):\n",
    "    \"\"\"Computes Dict of counts of words.\n",
    "    \n",
    "    Computes the number of times a word is on a document.\n",
    "    \"\"\"\n",
    "    vocab = defaultdict(float)\n",
    "    for content in list_of_content:\n",
    "        for line in content:\n",
    "            line = text_to_wordlist(line.strip())\n",
    "            words = set(line)\n",
    "            for word in words:\n",
    "                vocab[word] += 1\n",
    "    return vocab       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctZjpFbJZLQY"
   },
   "source": [
    "## Split train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9J3u1spZLQZ"
   },
   "outputs": [],
   "source": [
    "questions_cols=['question1','question2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OObuA0T4ZLQb"
   },
   "outputs": [],
   "source": [
    "X = train_df[questions_cols]\n",
    "Y = train_df['is_duplicate']\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jJSpof9qsV_P"
   },
   "outputs": [],
   "source": [
    "X_test = test_df[questions_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdgR2QH6ZLQk"
   },
   "outputs": [],
   "source": [
    "X_train1, X_train2 = np.array(X_train.question1), np.array(X_train.question2)\n",
    "X_val1, X_val2 = np.array(X_val.question1), np.array(X_val.question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lZd1NtW9sjeF"
   },
   "outputs": [],
   "source": [
    "X_test1, X_test2 = np.array(X_test.question1), np.array(X_test.question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L2cKQ8WZZLQq",
    "outputId": "ef922970-b1b5-4771-dee8-06196a1d8b6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_len = np.array([len(x.split()) for x in X_train2])\n",
    "np.percentile(x_len, 95) # let set the max sequence len to N=22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBpoA3ZiZLQt"
   },
   "source": [
    "### Computing vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLn9Wn2mZLQt"
   },
   "outputs": [],
   "source": [
    "# getting vocab from training sets\n",
    "word_count = get_vocab([X_train1,X_train2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "2IUsMWqeZLQx",
    "outputId": "ede0a485-ff9a-48ae-9e9d-114d36db03d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78051"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_count.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kca1lujOZLRD"
   },
   "source": [
    "## Use pretrained word embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "id": "dUpcpR8IZLRF",
    "outputId": "5f073685-3709-41a2-f91e-97b7bf66dee5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
      ", 0.013441 0.23682 -0.16899 0.40951 0.63812 0.47709 -0.42852 -0.55641 -0.364 -0.23938 0.13001 -0.063734 -0.39575 -0.48162 0.23291 0.090201 -0.13324 0.078639 -0.41634 -0.15428 0.10068 0.48891 0.31226 -0.1252 -0.037512 -1.5179 0.12612 -0.02442 -0.042961 -0.28351 3.5416 -0.11956 -0.014533 -0.1499 0.21864 -0.33412 -0.13872 0.31806 0.70358 0.44858 -0.080262 0.63003 0.32111 -0.46765 0.22786 0.36034 -0.37818 -0.56657 0.044691 0.30392\n"
     ]
    }
   ],
   "source": [
    "! head -2 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JPHzwkadZLRI"
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile=\"glove.6B.50d.txt\"):\n",
    "    \"\"\" Loads word vectors into a dictionary.\"\"\"\n",
    "    f = open(gloveFile,'r')\n",
    "    word_vecs = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        word_vecs[word] = np.array([float(val) for val in splitLine[1:]])\n",
    "    return word_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TLumeiAKZLRL"
   },
   "outputs": [],
   "source": [
    "word_vecs = loadGloveModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IjPNxWuNZLRN",
    "outputId": "ddd4e414-acfb-43bf-d073-14dcf1743636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000 78051\n"
     ]
    }
   ],
   "source": [
    "print(len(word_vecs.keys()), len(word_count.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fEZHdlvWZLRn"
   },
   "outputs": [],
   "source": [
    "def delete_rare_words(word_vecs, word_count, min_df=4):\n",
    "    \"\"\" Deletes rare words from word_count\n",
    "    \n",
    "    Deletes words from word_count if they are not in word_vecs\n",
    "    and don't have at least min_df occurrencies in word_count.\n",
    "    \"\"\"\n",
    "    words_delete = []\n",
    "    for word in word_count:\n",
    "        if word_count[word] < min_df and word not in word_vecs:\n",
    "            words_delete.append(word)\n",
    "    for word in words_delete: word_count.pop(word)\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BEJWR6CiZLRp"
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_vecs, word_count, min_df=4, emb_size=50):\n",
    "    \"\"\"Creates embedding matrix from word vectors. \"\"\"\n",
    "    word_count = delete_rare_words(word_vecs, word_count, min_df)\n",
    "    V = len(word_count.keys()) + 2\n",
    "    vocab2index = {}\n",
    "    W = np.zeros((V, emb_size), dtype=\"float32\")\n",
    "    vocab = [\"\", \"UNK\"]\n",
    "    # adding a vector for padding\n",
    "    W[0] = np.zeros(emb_size, dtype='float32')\n",
    "    # adding a vector for rare words \n",
    "    W[1] = np.random.uniform(-0.25, 0.25, emb_size)\n",
    "    vocab2index[\"UNK\"] = 1\n",
    "    i = 2\n",
    "    for word in word_count:\n",
    "        if word in word_vecs:\n",
    "            W[i] = word_vecs[word]\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1\n",
    "        else:\n",
    "            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n",
    "            vocab2index[word] = i\n",
    "            vocab.append(word)\n",
    "            i += 1   \n",
    "    return W, np.array(vocab), vocab2index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga_mng0rZLR1"
   },
   "outputs": [],
   "source": [
    "pretrained_weight, vocab, vocab2index = create_embedding_matrix(word_vecs, word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tmsuV5e1ZLR5",
    "outputId": "ce3489ec-da0e-49b1-c8fe-ed057a83244c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58626"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pretrained_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqTqiCmmZLSP"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nMHX39lLZLSP"
   },
   "outputs": [],
   "source": [
    "def encode_sentence(sentence, vocab2index, N=22, padding_start=True):\n",
    "    x = text_to_wordlist(sentence)\n",
    "    enc = np.zeros(N, dtype=np.int32)\n",
    "    enc1 = np.array([vocab2index.get(w, vocab2index[\"UNK\"]) for w in x])\n",
    "    l = min(N, len(enc1))\n",
    "    if padding_start:\n",
    "        enc[:l] = enc1[:l]\n",
    "    else:\n",
    "        enc[N-l:] = enc1[:l]\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdA8HfgYZLST"
   },
   "outputs": [],
   "source": [
    "class QuoraDataset(Dataset):\n",
    "    def __init__(self, X1,X2, y):\n",
    "        self.x1 = [encode_sentence(X,vocab2index) for X in X1]\n",
    "        self.x2 = [encode_sentence(X,vocab2index) for X in X2]\n",
    "        self.y = np.array(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x1[idx], self.x2[idx], self.y[idx]\n",
    "    \n",
    "train_ds = QuoraDataset(X_train1, X_train2, Y_train)\n",
    "valid_ds = QuoraDataset(X_val1, X_val2, Y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BhOiWYS2tKdc"
   },
   "outputs": [],
   "source": [
    "class QuoraDatasetTest(Dataset):\n",
    "    def __init__(self, X1,X2):\n",
    "        self.x1 = [encode_sentence(X,vocab2index) for X in X1]\n",
    "        self.x2 = [encode_sentence(X,vocab2index) for X in X2]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x1[idx], self.x2[idx]\n",
    "\n",
    "test_ds = QuoraDatasetTest(X_test1, X_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rekaIAw0ZLSx"
   },
   "outputs": [],
   "source": [
    "class GRUModel(torch.nn.Module) :\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights=None) :\n",
    "        super(GRUModel,self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        if glove_weights is not None:\n",
    "            self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n",
    "            self.embeddings.weight.requires_grad = False ## freeze embeddings\n",
    "            \n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x = torch.cat([x1, x2], dim=1)\n",
    "        x = self.embeddings(x)\n",
    "        x = self.dropout(x)\n",
    "        out_pack, ht = self.gru(x)\n",
    "        return self.linear(ht[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMyrzh-Tcf4Q"
   },
   "outputs": [],
   "source": [
    "def save_model(m, p): torch.save(m.state_dict(), p)\n",
    "    \n",
    "def load_model(m, p): m.load_state_dict(torch.load(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MMsLMfO8ckqD"
   },
   "outputs": [],
   "source": [
    "def cosine_segment(start_lr, end_lr, iterations):\n",
    "    i = np.arange(iterations)\n",
    "    c_i = 1 + np.cos(i*np.pi/iterations)\n",
    "    return end_lr + (start_lr - end_lr)/2 *c_i\n",
    "\n",
    "def get_cosine_triangular_lr(max_lr, iterations, div_start=5, div_end=5):\n",
    "    min_start, min_end = max_lr/div_start, max_lr/div_end\n",
    "    iter1 = int(0.3*iterations)\n",
    "    iter2 = iterations - iter1\n",
    "    segs = [cosine_segment(min_start, max_lr, iter1), cosine_segment(max_lr, min_end, iter2)]\n",
    "    return np.concatenate(segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kg2xThe-ZLS7"
   },
   "outputs": [],
   "source": [
    "def update_optimizer(optimizer, lr):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8dMLTA6zZLTE"
   },
   "outputs": [],
   "source": [
    "def train_epocs(model, optimizer, train_dl, val_dl, max_lr = 0.05, epochs=10):\n",
    "    iterations = epochs*len(train_dl)\n",
    "    idx = 0\n",
    "    best_val_loss = 100\n",
    "    lrs = get_cosine_triangular_lr(max_lr, iterations)\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        sum_loss = 0.0\n",
    "        total = 0\n",
    "        for x1, x2, y in train_dl:\n",
    "            update_optimizer(optimizer, lrs[idx])\n",
    "            x1 = x1.long().cuda()\n",
    "            x2 = x2.long().cuda()\n",
    "            y = y.float().cuda()\n",
    "            y_pred = model(x1, x2)\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()*y.shape[0]\n",
    "            total += y.shape[0]\n",
    "            idx += 1\n",
    "        val_loss, val_acc = val_metrics(model, val_dl)\n",
    "        if i % 5 == 1:\n",
    "            print(\"train loss %.3f val loss %.3f and val accuracy %.3f\" % (sum_loss/total, val_loss, val_acc))\n",
    "\n",
    "        if best_val_loss > val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            path = \"model_{0}_loss_{1:.3f}.pth\".format('quora', val_loss) \n",
    "            save_model(model, path)\n",
    "            print(path)\n",
    "    print(\"Best valid loss: {:.3f}\".format(best_val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLvWMxc-ZLTK"
   },
   "outputs": [],
   "source": [
    "def val_metrics(model, valid_dl):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    sum_loss = 0.0\n",
    "    for x1, x2, y in valid_dl:\n",
    "        x1 = x1.long().cuda()\n",
    "        x2 = x2.long().cuda()\n",
    "        y = y.float().unsqueeze(1).cuda()\n",
    "        y_hat = model(x1, x2)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_hat, y)\n",
    "        y_pred = y_hat > 0\n",
    "        correct += (y_pred.float() == y).float().sum()\n",
    "        total += y.shape[0]\n",
    "        sum_loss += loss.item()*y.shape[0]\n",
    "    return sum_loss/total, correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kP3ebLz1ZLTa"
   },
   "outputs": [],
   "source": [
    "batch_size = 3000\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=batch_size)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHKBeabtZLTm"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(pretrained_weight)\n",
    "model = GRUModel(vocab_size, 50, 50,glove_weights=pretrained_weight).cuda()\n",
    "\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "70u0kEfdZLTq",
    "outputId": "af82b617-2577-44ac-b3d6-1c908bd2b380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_quora_loss_0.578.pth\n",
      "train loss 0.572 val loss 0.597 and val accuracy 0.665\n",
      "model_quora_loss_0.568.pth\n",
      "model_quora_loss_0.559.pth\n",
      "Best valid loss: 0.559\n"
     ]
    }
   ],
   "source": [
    "train_epocs(model, optimizer, train_dl, valid_dl, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FtK-2A6wgY2G"
   },
   "outputs": [],
   "source": [
    "model.embeddings.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "id": "6NgjR8xDZLTv",
    "outputId": "07aaf22c-d8c1-4430-a938-9a88e35efdb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_quora_loss_0.548.pth\n",
      "train loss 0.540 val loss 0.540 and val accuracy 0.722\n",
      "model_quora_loss_0.540.pth\n",
      "model_quora_loss_0.539.pth\n",
      "model_quora_loss_0.531.pth\n",
      "model_quora_loss_0.526.pth\n",
      "model_quora_loss_0.518.pth\n",
      "train loss 0.501 val loss 0.518 and val accuracy 0.737\n",
      "model_quora_loss_0.518.pth\n",
      "model_quora_loss_0.508.pth\n",
      "model_quora_loss_0.504.pth\n",
      "model_quora_loss_0.500.pth\n",
      "train loss 0.462 val loss 0.496 and val accuracy 0.752\n",
      "model_quora_loss_0.496.pth\n",
      "model_quora_loss_0.484.pth\n",
      "model_quora_loss_0.482.pth\n",
      "model_quora_loss_0.481.pth\n",
      "train loss 0.423 val loss 0.493 and val accuracy 0.754\n",
      "model_quora_loss_0.464.pth\n",
      "train loss 0.389 val loss 0.485 and val accuracy 0.767\n",
      "model_quora_loss_0.460.pth\n",
      "train loss 0.361 val loss 0.467 and val accuracy 0.790\n",
      "train loss 0.339 val loss 0.481 and val accuracy 0.789\n",
      "train loss 0.319 val loss 0.488 and val accuracy 0.794\n",
      "train loss 0.304 val loss 0.491 and val accuracy 0.800\n",
      "train loss 0.292 val loss 0.508 and val accuracy 0.800\n",
      "train loss 0.281 val loss 0.505 and val accuracy 0.804\n",
      "train loss 0.270 val loss 0.518 and val accuracy 0.804\n",
      "train loss 0.261 val loss 0.531 and val accuracy 0.804\n",
      "train loss 0.254 val loss 0.550 and val accuracy 0.803\n",
      "train loss 0.247 val loss 0.542 and val accuracy 0.808\n",
      "train loss 0.242 val loss 0.552 and val accuracy 0.805\n",
      "train loss 0.236 val loss 0.560 and val accuracy 0.808\n",
      "train loss 0.232 val loss 0.564 and val accuracy 0.808\n",
      "train loss 0.229 val loss 0.580 and val accuracy 0.806\n",
      "train loss 0.226 val loss 0.580 and val accuracy 0.805\n",
      "Best valid loss: 0.460\n"
     ]
    }
   ],
   "source": [
    "#update_optimizer(optimizer, lr=0.001)\n",
    "train_epocs(model, optimizer, train_dl, valid_dl, max_lr = 0.01, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "io-0mKUSZLTz"
   },
   "outputs": [],
   "source": [
    "load_model(model, 'model_quora_loss_0.460.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlCkDyAYZLT5"
   },
   "outputs": [],
   "source": [
    "def test_score(model, test_dl):\n",
    "    model.eval()\n",
    "    m = nn.Sigmoid()\n",
    "    y_hat = []\n",
    "    for x1, x2 in test_dl:\n",
    "        x1 = x1.long().cuda()\n",
    "        x2 = x2.long().cuda()\n",
    "        out = m(model(x1, x2)).cuda()\n",
    "        y_hat.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    y_hat = np.vstack(y_hat)\n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AENH9PQSwWsd"
   },
   "outputs": [],
   "source": [
    "test_y = test_score(model, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n84UmPa_2BTs"
   },
   "outputs": [],
   "source": [
    "test_df['is_duplicate'] = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DbPil8ff2ALU"
   },
   "outputs": [],
   "source": [
    "submission = test_df[['test_id','is_duplicate']]\n",
    "#submission.set_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hXk3sRqJ2Ipv"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "pEglWaKu2lUS",
    "outputId": "3fd9f1e0-c9f9-4e9d-d0f7-aaa0fa7b3181"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.588951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.005010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.396398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.036706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.248485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563470</th>\n",
       "      <td>2345791</td>\n",
       "      <td>0.005506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563471</th>\n",
       "      <td>2345792</td>\n",
       "      <td>0.022244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563472</th>\n",
       "      <td>2345793</td>\n",
       "      <td>0.029853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563473</th>\n",
       "      <td>2345794</td>\n",
       "      <td>0.178204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3563474</th>\n",
       "      <td>2345795</td>\n",
       "      <td>0.131392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3563475 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         test_id  is_duplicate\n",
       "0              0      0.588951\n",
       "1              1      0.005010\n",
       "2              2      0.396398\n",
       "3              3      0.036706\n",
       "4              4      0.248485\n",
       "...          ...           ...\n",
       "3563470  2345791      0.005506\n",
       "3563471  2345792      0.022244\n",
       "3563472  2345793      0.029853\n",
       "3563473  2345794      0.178204\n",
       "3563474  2345795      0.131392\n",
       "\n",
       "[3563475 rows x 2 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "4F60LxUS6SfZ",
    "outputId": "18b0aa2b-8ea7-45c2-dd17-23f02abaf454"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "submission.drop(submission.tail(1217679).index,inplace=True) # drop last n rows\n",
    "submission.to_csv('submmision_fixed.csv', index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cdh7bwje9GzM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw22.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
